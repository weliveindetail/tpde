; NOTE: Assertions have been autogenerated by utils/update_tpde_llvm_test_checks.py UTC_ARGS: --tool tpde_llvm --default-march x86-64-v2 --version 5
; SPDX-FileCopyrightText: 2024 Tobias Schwarz <tobias.schwarz@tum.de>
;
; SPDX-License-Identifier: LicenseRef-Proprietary

; RUN: tpde_llvm --target=x86_64 %s | llvm-objdump -d -r --no-show-raw-insn --symbolize-operands --no-addresses --x86-asm-syntax=intel - | FileCheck %s -check-prefixes=X64,CHECK --enable-var-scope --dump-input always
; RUN: tpde_llvm --target=aarch64 %s | llvm-objdump -d -r --no-show-raw-insn --symbolize-operands --no-addresses - | FileCheck %s -check-prefixes=ARM64,CHECK --enable-var-scope --dump-input always

declare {i8, i1} @llvm.sadd.with.overflow.i8(i8, i8)
declare {i16, i1} @llvm.sadd.with.overflow.i16(i16, i16)
declare {i32, i1} @llvm.sadd.with.overflow.i32(i32, i32)
declare {i64, i1} @llvm.sadd.with.overflow.i64(i64, i64)
declare {i128, i1} @llvm.sadd.with.overflow.i128(i128, i128)

declare {i8, i1} @llvm.uadd.with.overflow.i8(i8, i8)
declare {i16, i1} @llvm.uadd.with.overflow.i16(i16, i16)
declare {i32, i1} @llvm.uadd.with.overflow.i32(i32, i32)
declare {i64, i1} @llvm.uadd.with.overflow.i64(i64, i64)
declare {i128, i1} @llvm.uadd.with.overflow.i128(i128, i128)

define i8 @uadd_i8_0(i8 %0, i8 %1) {
; X64-LABEL: uadd_i8_0>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    xor eax, eax
; X64-NEXT:    add dil, sil
; X64-NEXT:    setb al
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
;
; ARM64-LABEL: uadd_i8_0>:
; ARM64:         sub sp, sp, #0xb0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    and w0, w0, #0xff
; ARM64-NEXT:    add w0, w0, w1, uxtb
; ARM64-NEXT:    lsr w1, w0, #8
; ARM64-NEXT:    and x2, x0, #0xff
; ARM64-NEXT:    mov w0, w2
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xb0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i8, i1} @llvm.uadd.with.overflow.i8(i8 %0, i8 %1)
  %3 = extractvalue {i8, i1} %2, 0
  ret i8 %3
}

define i1 @uadd_i8_1(i8 %0, i8 %1) {
; X64-LABEL: uadd_i8_1>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    xor eax, eax
; X64-NEXT:    add dil, sil
; X64-NEXT:    setb al
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax + rax]
;
; ARM64-LABEL: uadd_i8_1>:
; ARM64:         sub sp, sp, #0xb0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    and w0, w0, #0xff
; ARM64-NEXT:    add w0, w0, w1, uxtb
; ARM64-NEXT:    lsr w1, w0, #8
; ARM64-NEXT:    and x2, x0, #0xff
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xb0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i8, i1} @llvm.uadd.with.overflow.i8(i8 %0, i8 %1)
  %3 = extractvalue {i8, i1} %2, 1
  ret i1 %3
}

define i16 @uadd_i16_0(i16 %0, i16 %1) {
; X64-LABEL: uadd_i16_0>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    xor eax, eax
; X64-NEXT:    add di, si
; X64-NEXT:    setb al
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
;
; ARM64-LABEL: uadd_i16_0>:
; ARM64:         sub sp, sp, #0xb0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    and w0, w0, #0xffff
; ARM64-NEXT:    add w0, w0, w1, uxth
; ARM64-NEXT:    lsr w1, w0, #16
; ARM64-NEXT:    and x2, x0, #0xffff
; ARM64-NEXT:    mov w0, w2
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xb0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i16, i1} @llvm.uadd.with.overflow.i16(i16 %0, i16 %1)
  %3 = extractvalue {i16, i1} %2, 0
  ret i16 %3
}

define i1 @uadd_i16_1(i16 %0, i16 %1) {
; X64-LABEL: uadd_i16_1>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x40
; X64-NEXT:    xor eax, eax
; X64-NEXT:    add di, si
; X64-NEXT:    setb al
; X64-NEXT:    add rsp, 0x40
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax + rax]
;
; ARM64-LABEL: uadd_i16_1>:
; ARM64:         sub sp, sp, #0xb0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    and w0, w0, #0xffff
; ARM64-NEXT:    add w0, w0, w1, uxth
; ARM64-NEXT:    lsr w1, w0, #16
; ARM64-NEXT:    and x2, x0, #0xffff
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xb0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i16, i1} @llvm.uadd.with.overflow.i16(i16 %0, i16 %1)
  %3 = extractvalue {i16, i1} %2, 1
  ret i1 %3
}

define i32 @uadd_i32_0(i32 %0, i32 %1) {
; X64-LABEL: uadd_i32_0>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x40
; X64-NEXT:    xor eax, eax
; X64-NEXT:    add edi, esi
; X64-NEXT:    setb al
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x40
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
;
; ARM64-LABEL: uadd_i32_0>:
; ARM64:         sub sp, sp, #0xb0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    adds w0, w0, w1
; ARM64-NEXT:    cset w1, hs
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xb0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i32, i1} @llvm.uadd.with.overflow.i32(i32 %0, i32 %1)
  %3 = extractvalue {i32, i1} %2, 0
  ret i32 %3
}

define i1 @uadd_i32_1(i32 %0, i32 %1) {
; X64-LABEL: uadd_i32_1>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x40
; X64-NEXT:    xor eax, eax
; X64-NEXT:    add edi, esi
; X64-NEXT:    setb al
; X64-NEXT:    add rsp, 0x40
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop word ptr [rax + rax]
;
; ARM64-LABEL: uadd_i32_1>:
; ARM64:         sub sp, sp, #0xc0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    adds w0, w0, w1
; ARM64-NEXT:    cset w1, hs
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xc0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i32, i1} @llvm.uadd.with.overflow.i32(i32 %0, i32 %1)
  %3 = extractvalue {i32, i1} %2, 1
  ret i1 %3
}

define i64 @uadd_i64_0(i64 %0, i64 %1) {
; X64-LABEL: uadd_i64_0>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x50
; X64-NEXT:    xor eax, eax
; X64-NEXT:    add rdi, rsi
; X64-NEXT:    setb al
; X64-NEXT:    mov rax, rdi
; X64-NEXT:    add rsp, 0x50
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop
;
; ARM64-LABEL: uadd_i64_0>:
; ARM64:         sub sp, sp, #0xc0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    adds x0, x0, x1
; ARM64-NEXT:    cset w1, hs
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xc0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i64, i1} @llvm.uadd.with.overflow.i64(i64 %0, i64 %1)
  %3 = extractvalue {i64, i1} %2, 0
  ret i64 %3
}

define i1 @uadd_i64_1(i64 %0, i64 %1) {
; X64-LABEL: uadd_i64_1>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x60
; X64-NEXT:    xor eax, eax
; X64-NEXT:    add rdi, rsi
; X64-NEXT:    setb al
; X64-NEXT:    add rsp, 0x60
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax + rax]
;
; ARM64-LABEL: uadd_i64_1>:
; ARM64:         sub sp, sp, #0xd0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    adds x0, x0, x1
; ARM64-NEXT:    cset w1, hs
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xd0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i64, i1} @llvm.uadd.with.overflow.i64(i64 %0, i64 %1)
  %3 = extractvalue {i64, i1} %2, 1
  ret i1 %3
}

define i128 @uadd_i128_0(i128 %0, i128 %1) {
; X64-LABEL: uadd_i128_0>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x70
; X64-NEXT:    mov rax, rdi
; X64-NEXT:    add rax, rdx
; X64-NEXT:    adc rsi, rcx
; X64-NEXT:    setb cl
; X64-NEXT:    movzx ecx, cl
; X64-NEXT:    mov rdx, rsi
; X64-NEXT:    add rsp, 0x70
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop dword ptr [rax]
;
; ARM64-LABEL: uadd_i128_0>:
; ARM64:         sub sp, sp, #0xe0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    adds x4, x0, x2
; ARM64-NEXT:    adcs x5, x1, x3
; ARM64-NEXT:    cset w6, hs
; ARM64-NEXT:    mov x0, x4
; ARM64-NEXT:    mov x1, x5
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xe0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i128, i1} @llvm.uadd.with.overflow.i128(i128 %0, i128 %1)
  %3 = extractvalue {i128, i1} %2, 0
  ret i128 %3
}

define i1 @uadd_i128_1(i128 %0, i128 %1) {
; X64-LABEL: uadd_i128_1>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x80
; X64-NEXT:    mov rax, rdi
; X64-NEXT:    add rax, rdx
; X64-NEXT:    adc rsi, rcx
; X64-NEXT:    setb cl
; X64-NEXT:    movzx ecx, cl
; X64-NEXT:    mov eax, ecx
; X64-NEXT:    add rsp, 0x80
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop
;
; ARM64-LABEL: uadd_i128_1>:
; ARM64:         sub sp, sp, #0xf0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    adds x4, x0, x2
; ARM64-NEXT:    adcs x5, x1, x3
; ARM64-NEXT:    cset w6, hs
; ARM64-NEXT:    mov w0, w6
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xf0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i128, i1} @llvm.uadd.with.overflow.i128(i128 %0, i128 %1)
  %3 = extractvalue {i128, i1} %2, 1
  ret i1 %3
}



define i8 @sadd_i8_0(i8 %0, i8 %1) {
; X64-LABEL: sadd_i8_0>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    xor eax, eax
; X64-NEXT:    add dil, sil
; X64-NEXT:    seto al
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
;
; ARM64-LABEL: sadd_i8_0>:
; ARM64:         sub sp, sp, #0xb0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    sxtb w0, w0
; ARM64-NEXT:    add w0, w0, w1, sxtb
; ARM64-NEXT:    cmp w0, w0, sxtb
; ARM64-NEXT:    and x1, x0, #0xff
; ARM64-NEXT:    cset w2, ne
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xb0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i8, i1} @llvm.sadd.with.overflow.i8(i8 %0, i8 %1)
  %3 = extractvalue {i8, i1} %2, 0
  ret i8 %3
}

define i1 @sadd_i8_1(i8 %0, i8 %1) {
; X64-LABEL: sadd_i8_1>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    xor eax, eax
; X64-NEXT:    add dil, sil
; X64-NEXT:    seto al
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax + rax]
;
; ARM64-LABEL: sadd_i8_1>:
; ARM64:         sub sp, sp, #0xb0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    sxtb w0, w0
; ARM64-NEXT:    add w0, w0, w1, sxtb
; ARM64-NEXT:    cmp w0, w0, sxtb
; ARM64-NEXT:    and x1, x0, #0xff
; ARM64-NEXT:    cset w2, ne
; ARM64-NEXT:    mov w0, w2
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xb0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i8, i1} @llvm.sadd.with.overflow.i8(i8 %0, i8 %1)
  %3 = extractvalue {i8, i1} %2, 1
  ret i1 %3
}

define i16 @sadd_i16_0(i16 %0, i16 %1) {
; X64-LABEL: sadd_i16_0>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x30
; X64-NEXT:    xor eax, eax
; X64-NEXT:    add di, si
; X64-NEXT:    seto al
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x30
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
;
; ARM64-LABEL: sadd_i16_0>:
; ARM64:         sub sp, sp, #0xb0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    sxth w0, w0
; ARM64-NEXT:    add w0, w0, w1, sxth
; ARM64-NEXT:    cmp w0, w0, sxth
; ARM64-NEXT:    and x1, x0, #0xffff
; ARM64-NEXT:    cset w2, ne
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xb0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i16, i1} @llvm.sadd.with.overflow.i16(i16 %0, i16 %1)
  %3 = extractvalue {i16, i1} %2, 0
  ret i16 %3
}

define i1 @sadd_i16_1(i16 %0, i16 %1) {
; X64-LABEL: sadd_i16_1>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x40
; X64-NEXT:    xor eax, eax
; X64-NEXT:    add di, si
; X64-NEXT:    seto al
; X64-NEXT:    add rsp, 0x40
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax + rax]
;
; ARM64-LABEL: sadd_i16_1>:
; ARM64:         sub sp, sp, #0xb0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    sxth w0, w0
; ARM64-NEXT:    add w0, w0, w1, sxth
; ARM64-NEXT:    cmp w0, w0, sxth
; ARM64-NEXT:    and x1, x0, #0xffff
; ARM64-NEXT:    cset w2, ne
; ARM64-NEXT:    mov w0, w2
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xb0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i16, i1} @llvm.sadd.with.overflow.i16(i16 %0, i16 %1)
  %3 = extractvalue {i16, i1} %2, 1
  ret i1 %3
}

define i32 @sadd_i32_0(i32 %0, i32 %1) {
; X64-LABEL: sadd_i32_0>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x40
; X64-NEXT:    xor eax, eax
; X64-NEXT:    add edi, esi
; X64-NEXT:    seto al
; X64-NEXT:    mov eax, edi
; X64-NEXT:    add rsp, 0x40
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
;
; ARM64-LABEL: sadd_i32_0>:
; ARM64:         sub sp, sp, #0xb0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    adds w0, w0, w1
; ARM64-NEXT:    cset w1, vs
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xb0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i32, i1} @llvm.sadd.with.overflow.i32(i32 %0, i32 %1)
  %3 = extractvalue {i32, i1} %2, 0
  ret i32 %3
}

define i1 @sadd_i32_1(i32 %0, i32 %1) {
; X64-LABEL: sadd_i32_1>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x40
; X64-NEXT:    xor eax, eax
; X64-NEXT:    add edi, esi
; X64-NEXT:    seto al
; X64-NEXT:    add rsp, 0x40
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop word ptr [rax + rax]
;
; ARM64-LABEL: sadd_i32_1>:
; ARM64:         sub sp, sp, #0xc0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    adds w0, w0, w1
; ARM64-NEXT:    cset w1, vs
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xc0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i32, i1} @llvm.sadd.with.overflow.i32(i32 %0, i32 %1)
  %3 = extractvalue {i32, i1} %2, 1
  ret i1 %3
}

define i64 @sadd_i64_0(i64 %0, i64 %1) {
; X64-LABEL: sadd_i64_0>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x50
; X64-NEXT:    xor eax, eax
; X64-NEXT:    add rdi, rsi
; X64-NEXT:    seto al
; X64-NEXT:    mov rax, rdi
; X64-NEXT:    add rsp, 0x50
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop
;
; ARM64-LABEL: sadd_i64_0>:
; ARM64:         sub sp, sp, #0xc0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    adds x0, x0, x1
; ARM64-NEXT:    cset w1, vs
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xc0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i64, i1} @llvm.sadd.with.overflow.i64(i64 %0, i64 %1)
  %3 = extractvalue {i64, i1} %2, 0
  ret i64 %3
}

define i1 @sadd_i64_1(i64 %0, i64 %1) {
; X64-LABEL: sadd_i64_1>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x60
; X64-NEXT:    xor eax, eax
; X64-NEXT:    add rdi, rsi
; X64-NEXT:    seto al
; X64-NEXT:    add rsp, 0x60
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax + rax]
;
; ARM64-LABEL: sadd_i64_1>:
; ARM64:         sub sp, sp, #0xd0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    adds x0, x0, x1
; ARM64-NEXT:    cset w1, vs
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xd0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i64, i1} @llvm.sadd.with.overflow.i64(i64 %0, i64 %1)
  %3 = extractvalue {i64, i1} %2, 1
  ret i1 %3
}

define i128 @sadd_i128_0(i128 %0, i128 %1) {
; X64-LABEL: sadd_i128_0>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x70
; X64-NEXT:    mov rax, rdi
; X64-NEXT:    add rax, rdx
; X64-NEXT:    adc rsi, rcx
; X64-NEXT:    seto cl
; X64-NEXT:    movzx ecx, cl
; X64-NEXT:    mov rdx, rsi
; X64-NEXT:    add rsp, 0x70
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
; X64-NEXT:    nop dword ptr [rax]
;
; ARM64-LABEL: sadd_i128_0>:
; ARM64:         sub sp, sp, #0xe0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    adds x4, x0, x2
; ARM64-NEXT:    adcs x5, x1, x3
; ARM64-NEXT:    cset w6, vs
; ARM64-NEXT:    mov x0, x4
; ARM64-NEXT:    mov x1, x5
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xe0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i128, i1} @llvm.sadd.with.overflow.i128(i128 %0, i128 %1)
  %3 = extractvalue {i128, i1} %2, 0
  ret i128 %3
}

define i1 @sadd_i128_1(i128 %0, i128 %1) {
; X64-LABEL: sadd_i128_1>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    sub rsp, 0x80
; X64-NEXT:    mov rax, rdi
; X64-NEXT:    add rax, rdx
; X64-NEXT:    adc rsi, rcx
; X64-NEXT:    seto cl
; X64-NEXT:    movzx ecx, cl
; X64-NEXT:    mov eax, ecx
; X64-NEXT:    add rsp, 0x80
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: sadd_i128_1>:
; ARM64:         sub sp, sp, #0xf0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    adds x4, x0, x2
; ARM64-NEXT:    adcs x5, x1, x3
; ARM64-NEXT:    cset w6, vs
; ARM64-NEXT:    mov w0, w6
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xf0
; ARM64-NEXT:    ret
; ARM64-NEXT:     ...
entry:
  %2 = call {i128, i1} @llvm.sadd.with.overflow.i128(i128 %0, i128 %1)
  %3 = extractvalue {i128, i1} %2, 1
  ret i1 %3
}
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; CHECK: {{.*}}
